{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this Notebook, We will convert the model artifact, created in previous notbooks and added to S3 Model artifact, to a TF lite model \n",
    "### Which is a nessecry step to deploy the model on the PSoC 6 board "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pathlib\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role = get_execution_role()\n",
    "sess = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands: ['yes' 'no']\n",
      "Number of total examples: 2000\n",
      "Number of examples per label: 1000\n",
      "Example file tensor: tf.Tensor(b'data/mini_speech_commands/yes/573cdb8a_nohash_0.wav', shape=(), dtype=string)\n",
      "Training set size 1600\n",
      "Validation set size 200\n",
      "Test set size 200\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "def decode_audio(audio_binary):\n",
    "  # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
    "  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
    "  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
    "  # Since all the data is single channel (mono), drop the `channels`\n",
    "  # axis from the array.\n",
    "  return tf.squeeze(audio, axis=-1)\n",
    "\n",
    "def get_label(file_path):\n",
    "  parts = tf.strings.split(\n",
    "      input=file_path,\n",
    "      sep=os.path.sep)\n",
    "  # Note: You'll use indexing here instead of tuple unpacking to enable this\n",
    "  # to work in a TensorFlow graph.\n",
    "  return parts[-2]\n",
    "\n",
    "def get_waveform_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  audio_binary = tf.io.read_file(file_path)\n",
    "  waveform = decode_audio(audio_binary)\n",
    "  return waveform, label\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "  # Zero-padding for an audio waveform with less than 16,000 samples.\n",
    "  input_len = 16000\n",
    "  waveform = waveform[:input_len]\n",
    "  zero_padding = tf.zeros(\n",
    "      [16000] - tf.shape(waveform),\n",
    "      dtype=tf.float32)\n",
    "  # Cast the waveform tensors' dtype to float32.\n",
    "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "  # Concatenate the waveform with `zero_padding`, which ensures all audio\n",
    "  # clips are of the same length.\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=325, fft_length =78 )\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram\n",
    "\n",
    "def get_spectrogram_and_label_id(audio, label):\n",
    "  spectrogram = get_spectrogram(audio)\n",
    "  label_id = tf.argmax(label == commands)\n",
    "  return spectrogram, label_id\n",
    "\n",
    "def preprocess_dataset(files):\n",
    "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "  output_ds = files_ds.map(\n",
    "      map_func=get_waveform_and_label,\n",
    "      num_parallel_calls=AUTOTUNE)\n",
    "  output_ds = output_ds.map(\n",
    "      map_func=get_spectrogram_and_label_id,\n",
    "      num_parallel_calls=AUTOTUNE)\n",
    "  return output_ds\n",
    "\n",
    "# Importing smaller data\n",
    "DATASET_PATH = 'data/mini_speech_commands'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "if not data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'mini_speech_commands.zip',\n",
    "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data')\n",
    "\n",
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != 'README.md']\n",
    "commands = commands[commands !='.ipynb_checkpoints']\n",
    "print('Commands:', commands)\n",
    "\n",
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Number of examples per label:',\n",
    "      len(tf.io.gfile.listdir(str(data_dir/commands[1]))))\n",
    "print('Example file tensor:', filenames[0])\n",
    "\n",
    "train_files = filenames[:int(0.8*len(filenames))]\n",
    "val_files = filenames[int(0.8*len(filenames)): int(0.8*len(filenames)) + int(0.1*len(filenames))]\n",
    "test_files = filenames[-int(0.1*len(filenames)):]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = preprocess_dataset(train_files)\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "test_ds_batch = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# copy a pretrained model from a public bucket to your default bucket\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"sagemaker-studio-062044820001-7qbctb3w94p\"\n",
    "key = \"Training/models/1/tensorflow-training-2022-05-11-13-33-00-336/output/model.tar.gz\"\n",
    "s3.download_file(bucket, key, \"model/model.tar.gz\")\n",
    "\n",
    "# Untar the model\n",
    "import tarfile\n",
    "\n",
    "fname = \"model/model.tar.gz\"\n",
    "if fname.endswith(\"tar.gz\"):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall(\"model\")\n",
    "    tar.close()\n",
    "elif fname.endswith(\"tar\"):\n",
    "    tar = tarfile.open(fname, \"r:\")\n",
    "    tar.extractall(\"model\")\n",
    "    tar.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/speech-recognition-AWS\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'micro-speech-example/models/tflite/model.tflite'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_DIR = 'micro-speech-example/models'\n",
    "\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "MODEL_TFLITE = MODELS_DIR + '/tflite/model.tflite'\n",
    "FLOAT_MODEL_TFLITE = MODELS_DIR + '/tflite/float_model.tflite'\n",
    "MODEL_TFLITE_MICRO = MODELS_DIR + '/tflite/model.cc'\n",
    "SAVED_MODEL = MODELS_DIR + '/1'\n",
    "\n",
    "MODEL_TFLITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using `TFLiteConverter` to convert TF model to TF-lite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model is 17044 bytes\n"
     ]
    }
   ],
   "source": [
    "## Convert the model to the TensorFlow Lite format without quantization\n",
    "\n",
    "float_converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=SAVED_MODEL)\n",
    "float_tflite_model = float_converter.convert()\n",
    "float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "print(\"Float model is %d bytes\" % float_tflite_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model is 6536 bytes\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=SAVED_MODEL)\n",
    "\n",
    "def representative_dataset():\n",
    "  for data, _ in train_ds.take(500):\n",
    "    yield [data]\n",
    "# Set the optimization flag.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Enforce integer only quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "print(\"Quantized model is %d bytes\" % tflite_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "x_test = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1, 49, 40, 1)\n",
      "input_details:   {'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 49, 40,  1], dtype=int32), 'shape_signature': array([ 1, 49, 40,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "200\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "Float model accuracy is 23.500000% (Number of test samples=200)\n",
      "(200, 1, 49, 40, 1)\n",
      "input_details:   {'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 49, 40,  1], dtype=int32), 'shape_signature': array([ 1, 49, 40,  1], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.32568514347076416, 63), 'quantization_parameters': {'scales': array([0.32568514], dtype=float32), 'zero_points': array([63], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "200\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "Quantized model accuracy is 39.000000% (Number of test samples=200)\n"
     ]
    }
   ],
   "source": [
    "# Helper function to run inference\n",
    "def run_tflite_inference(tflite_model_path, x_test, test_labels, model_type=\"Float\" ):\n",
    "    \n",
    "    # Prepare the test data\n",
    "    \n",
    "    x_test_ = x_test.copy()\n",
    "    x_test_ = np.expand_dims(x_test_, axis=1).astype(np.float32)\n",
    "    print(x_test_.shape)\n",
    "    \n",
    "    # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    print(\"input_details:  \", input_details)\n",
    "    \n",
    "    # For quantized models, manually quantize the input data from float to integer\n",
    "    if model_type == \"Quantized\":\n",
    "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "        x_test_ = x_test_ / input_scale + input_zero_point\n",
    "        x_test_ = x_test_.astype(input_details[\"dtype\"])\n",
    "\n",
    "    correct_predictions = 0\n",
    "    print(len(x_test_))\n",
    "    for i in range(len(x_test_)):\n",
    "        interpreter.set_tensor(input_details[\"index\"], x_test_[i])\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "        \n",
    "        top_prediction = output.argmax()\n",
    "        print((top_prediction == test_labels[i]))\n",
    "        correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "    print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "      model_type, (correct_predictions * 100) / len(x_test_), len(x_test_)))\n",
    "\n",
    "# Compute float model accuracy\n",
    "run_tflite_inference(FLOAT_MODEL_TFLITE, x_test, test_labels)\n",
    "\n",
    "# Compute quantized model accuracy\n",
    "run_tflite_inference(MODEL_TFLITE, x_test, test_labels, model_type='Quantized')\n",
    "\n",
    "# def predict_tflite(tflite_model, x_test):\n",
    "#     # Prepare the test data\n",
    "#     x_test_ = x_test.copy()\n",
    "#     x_test_ = x_test_.reshape((x_test.size, 1))\n",
    "#     x_test_ = x_test_.astype(np.float32)\n",
    "\n",
    "#     # Initialize the TFLite interpreter\n",
    "#     interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "#     interpreter.allocate_tensors()\n",
    "\n",
    "#     input_details = interpreter.get_input_details()[0]\n",
    "#     output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "#     # If required, quantize the input layer (from float to integer)\n",
    "#     input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "#     if (input_scale, input_zero_point) != (0.0, 0):\n",
    "#         x_test_ = x_test_ / input_scale + input_zero_point\n",
    "#         x_test_ = x_test_.astype(input_details[\"dtype\"])\n",
    "\n",
    "#     # Invoke the interpreter\n",
    "#     y_pred = np.empty(x_test_.size, dtype=output_details[\"dtype\"])\n",
    "#     for i in range(len(x_test_)):\n",
    "#         interpreter.set_tensor(input_details[\"index\"], [x_test_[i]])\n",
    "#         interpreter.invoke()\n",
    "#         y_pred[i] = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "#     # If required, dequantized the output layer (from integer to float)\n",
    "#     output_scale, output_zero_point = output_details[\"quantization\"]\n",
    "#     if (output_scale, output_zero_point) != (0.0, 0):\n",
    "#         y_pred = y_pred.astype(np.float32)\n",
    "#         y_pred = (y_pred - output_zero_point) * output_scale\n",
    "\n",
    "#     return y_pred\n",
    "\n",
    "# def evaluate_tflite(tflite_model, x_test, y_true):\n",
    "#     global model\n",
    "#     y_pred = predict_tflite(tflite_model, x_test)\n",
    "#     loss_function = tf.keras.losses.get(model.loss)\n",
    "#     loss = loss_function(y_true, y_pred).numpy()\n",
    "#     return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
