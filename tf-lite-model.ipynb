{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this Notebook, We will convert the model artifact, created in previous notbooks and added to S3 Model artifact, to a TF lite model \n",
    "### Which is a nessecry step to deploy the model on the PSoC 6 board "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands: ['yes' 'no']\n",
      "Number of total examples: 2000\n",
      "Number of examples per label: 1000\n",
      "Example file tensor: tf.Tensor(b'data/mini_speech_commands/yes/9b5815cd_nohash_0.wav', shape=(), dtype=string)\n",
      "Training set size 1600\n",
      "Validation set size 200\n",
      "Test set size 200\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pathlib\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role = get_execution_role()\n",
    "sess = Session()\n",
    "\n",
    "def decode_audio(audio_binary):\n",
    "  # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
    "  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
    "  audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
    "  # Since all the data is single channel (mono), drop the `channels`\n",
    "  # axis from the array.\n",
    "  return tf.squeeze(audio, axis=-1)\n",
    "\n",
    "def get_label(file_path):\n",
    "  parts = tf.strings.split(\n",
    "      input=file_path,\n",
    "      sep=os.path.sep)\n",
    "  # Note: You'll use indexing here instead of tuple unpacking to enable this\n",
    "  # to work in a TensorFlow graph.\n",
    "  return parts[-2]\n",
    "\n",
    "def get_waveform_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  audio_binary = tf.io.read_file(file_path)\n",
    "  waveform = decode_audio(audio_binary)\n",
    "  return waveform, label\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "  # Zero-padding for an audio waveform with less than 16,000 samples.\n",
    "  input_len = 16000\n",
    "  waveform = waveform[:input_len]\n",
    "  zero_padding = tf.zeros(\n",
    "      [16000] - tf.shape(waveform),\n",
    "      dtype=tf.float32)\n",
    "  # Cast the waveform tensors' dtype to float32.\n",
    "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "  # Concatenate the waveform with `zero_padding`, which ensures all audio\n",
    "  # clips are of the same length.\n",
    "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      equal_length, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram\n",
    "\n",
    "def get_spectrogram_and_label_id(audio, label):\n",
    "  spectrogram = get_spectrogram(audio)\n",
    "  label_id = tf.argmax(label == commands)\n",
    "  return spectrogram, label_id\n",
    "\n",
    "def preprocess_dataset(files):\n",
    "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "  output_ds = files_ds.map(\n",
    "      map_func=get_waveform_and_label,\n",
    "      num_parallel_calls=AUTOTUNE)\n",
    "  output_ds = output_ds.map(\n",
    "      map_func=get_spectrogram_and_label_id,\n",
    "      num_parallel_calls=AUTOTUNE)\n",
    "  return output_ds\n",
    "\n",
    "# Importing smaller data\n",
    "DATASET_PATH = 'data/mini_speech_commands'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "if not data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'mini_speech_commands.zip',\n",
    "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data')\n",
    "\n",
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "commands = commands[commands != 'README.md']\n",
    "commands = commands[commands !='.ipynb_checkpoints']\n",
    "print('Commands:', commands)\n",
    "\n",
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Number of examples per label:',\n",
    "      len(tf.io.gfile.listdir(str(data_dir/commands[1]))))\n",
    "print('Example file tensor:', filenames[0])\n",
    "\n",
    "train_files = filenames[:int(0.8*len(filenames))]\n",
    "val_files = filenames[int(0.8*len(filenames)): int(0.8*len(filenames)) + int(0.1*len(filenames))]\n",
    "test_files = filenames[-int(0.1*len(filenames)):]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = preprocess_dataset(train_files)\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "test_ds_batch = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "# copy a pretrained model from a public bucket to your default bucket\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"sagemaker-studio-062044820001-7qbctb3w94p\"\n",
    "key = \"Training/models/1/tensorflow-training-2022-04-27-21-00-26-837/output/model.tar.gz\"\n",
    "s3.download_file(bucket, key, \"model/model.tar.gz\")\n",
    "\n",
    "# Untar the model\n",
    "import tarfile\n",
    "\n",
    "fname = \"model/model.tar.gz\"\n",
    "if fname.endswith(\"tar.gz\"):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall(\"model\")\n",
    "    tar.close()\n",
    "elif fname.endswith(\"tar\"):\n",
    "    tar = tarfile.open(fname, \"r:\")\n",
    "    tar.extractall(\"model\")\n",
    "    tar.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/tflite/model.tflite'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_DIR = 'model'\n",
    "\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "MODEL_TFLITE = MODELS_DIR + '/tflite/model.tflite'\n",
    "FLOAT_MODEL_TFLITE = MODELS_DIR + '/tflite/float_model.tflite'\n",
    "MODEL_TFLITE_MICRO = MODELS_DIR + '/tflite/model.cc'\n",
    "SAVED_MODEL = MODELS_DIR + '/1'\n",
    "\n",
    "MODEL_TFLITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using `TFLiteConverter` to convert TF model to TF-lite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model is 6502428 bytes\n"
     ]
    }
   ],
   "source": [
    "## Convert the model to the TensorFlow Lite format without quantization\n",
    "\n",
    "float_converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=SAVED_MODEL)\n",
    "float_tflite_model = float_converter.convert()\n",
    "float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "print(\"Float model is %d bytes\" % float_tflite_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model is 1631416 bytes\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=SAVED_MODEL)\n",
    "\n",
    "def representative_dataset():\n",
    "  for data, _ in train_ds.take(500):\n",
    "    yield [data]\n",
    "# Set the optimization flag.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Enforce integer only quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "print(\"Quantized model is %d bytes\" % tflite_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference\n",
    "def run_tflite_inference(tflite_model_path, test_ds_batch, model_type=\"Float\" ):\n",
    "\n",
    "    # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    test_audio = []\n",
    "    test_labels = []\n",
    "\n",
    "    for audio, label in test_ds:\n",
    "          test_audio.append(audio.numpy())\n",
    "          test_labels.append(label.numpy())\n",
    "\n",
    "    \n",
    "    # For quantized models, manually quantize the input data from float to integer\n",
    "    if model_type == \"Quantized\":\n",
    "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "        test_ds_batch = test_ds_batch / input_scale + input_zero_point\n",
    "        test_ds_batch = test_ds_batch.astype(input_details[\"dtype\"])\n",
    "\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(test_data)):\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "        top_prediction = output.argmax()\n",
    "        correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "    print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))\n",
    "    # Compute float model accuracy\n",
    "# run_tflite_inference(FLOAT_MODEL_TFLITE)\n",
    "\n",
    "# Compute quantized model accuracy\n",
    "run_tflite_inference(MODEL_TFLITE, test_ds_batch,model_type='Quantized')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
